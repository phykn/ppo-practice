{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "render_mode = None\n",
    "env = gym.make(env_name, render_mode = render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02752061  0.0366879  -0.00013642  0.02821334]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "print(observation)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    action_sample = env.action_space.sample()\n",
    "    print(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: [ 0.02825437  0.2318118   0.00042785 -0.26451263]\n",
      "reward: 1.0\n",
      "terminated: False\n",
      "truncated: False\n",
      "info: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python_env\\pytorch\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action_sample)\n",
    "\n",
    "print(f\"observation: {observation}\")\n",
    "print(f\"reward: {reward}\")\n",
    "print(f\"terminated: {terminated}\")\n",
    "print(f\"truncated: {truncated}\")\n",
    "print(f\"info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        self.actions.clear()\n",
    "        self.states.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.state_values.clear()\n",
    "        self.is_terminals.clear()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            state_dim = 4, \n",
    "            action_dim = 2, \n",
    "            d_model = 64,\n",
    "            is_continuous_action_space = False, \n",
    "            action_std_init = 0.6,\n",
    "            device = \"cpu\"\n",
    "        ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.is_continuous_action_space = is_continuous_action_space\n",
    "        self.device = device\n",
    "\n",
    "        if is_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full(\n",
    "                size = (action_dim, ),\n",
    "                fill_value = action_std_init ** 2,\n",
    "                device = device\n",
    "            )\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, action_dim)\n",
    "        ).to(device)\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        ).to(device)\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.is_continuous_action_space:\n",
    "            self.action_var = torch.full(\n",
    "                size = (self.action_dim, ), \n",
    "                fill_value = new_action_std ** 2,\n",
    "                device = self.device\n",
    "            )\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state):\n",
    "        batch_size = len(state)\n",
    "\n",
    "        if self.is_continuous_action_space:\n",
    "            dist = MultivariateNormal(\n",
    "                loc = self.actor(state), \n",
    "                covariance_matrix = torch.diag(self.action_var)\n",
    "            )\n",
    "        else:\n",
    "            dist = Categorical(\n",
    "                logits = self.actor(state)\n",
    "            )\n",
    "\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        action = action.reshape(batch_size, -1)\n",
    "        log_prob = log_prob.reshape(batch_size, -1)\n",
    "        state_value = state_value.reshape(batch_size, -1)\n",
    "        return action.detach(), log_prob.detach(), state_value.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        batch_size = len(state)\n",
    "\n",
    "        if self.is_continuous_action_space:\n",
    "            dist = MultivariateNormal(\n",
    "                loc = self.actor(state), \n",
    "                covariance_matrix = torch.diag(self.action_var)\n",
    "            )\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        else:\n",
    "            dist = Categorical(\n",
    "                logits = self.actor(state)\n",
    "            )\n",
    "            log_prob = dist.log_prob(action[:, 0])\n",
    "        \n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        log_prob = log_prob.reshape(batch_size, -1)\n",
    "        state_value = state_value.reshape(batch_size, -1)\n",
    "        dist_entropy = dist_entropy.reshape(batch_size, -1)\n",
    "        return log_prob, state_value, dist_entropy\n",
    "    \n",
    "class PPO:\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim = 4, \n",
    "            action_dim = 2, \n",
    "            d_model = 64,\n",
    "            is_continuous_action_space = False,\n",
    "            action_std_init = 0.6,\n",
    "            K_epochs = 80,\n",
    "            eps_clip = 0.2,\n",
    "            gamma = 0.99,\n",
    "            lr_actor = 0.0003,\n",
    "            lr_critic = 0.001,\n",
    "            device = \"cpu\"\n",
    "    ):\n",
    "        if is_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.is_continuous_action_space = is_continuous_action_space\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(\n",
    "            state_dim = state_dim, \n",
    "            action_dim = action_dim, \n",
    "            d_model = d_model,\n",
    "            is_continuous_action_space = is_continuous_action_space, \n",
    "            action_std_init = action_std_init,\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "        self.policy_old = ActorCritic(\n",
    "            state_dim = state_dim, \n",
    "            action_dim = action_dim, \n",
    "            d_model = d_model,\n",
    "            is_continuous_action_space = is_continuous_action_space, \n",
    "            action_std_init = action_std_init,\n",
    "            device = device\n",
    "        )\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {\"params\": self.policy.actor.parameters(), \"lr\": lr_actor},\n",
    "            {\"params\": self.policy.critic.parameters(), \"lr\": lr_critic}\n",
    "        ])\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.is_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        if self.is_continuous_action_space:\n",
    "            new_action_std = self.action_std - action_std_decay_rate\n",
    "            new_action_std = round(new_action_std, 4)\n",
    "            new_action_std = min_action_std if new_action_std <= min_action_std else new_action_std\n",
    "            self.set_action_std(new_action_std)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action, log_prob, state_value = self.policy_old.act(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.log_probs.append(log_prob)\n",
    "        self.buffer.state_values.append(state_value)\n",
    "\n",
    "        return action.detach().cpu().numpy()\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_old.state_dict(), path)\n",
    "\n",
    "    def update(self):\n",
    "        # monte carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # normalize the rewards\n",
    "        rewards = torch.tensor(rewards).float()\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        rewards = rewards.reshape(len(rewards), -1)\n",
    "        rewards = rewards.to(self.device)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.cat(self.buffer.states)\n",
    "        old_actions = torch.cat(self.buffer.actions)\n",
    "        old_log_probs = torch.cat(self.buffer.log_probs)\n",
    "        old_state_values = torch.cat(self.buffer.state_values)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards - old_state_values\n",
    "\n",
    "        total_loss = []\n",
    "        # optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            log_prob, state_value, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(log_prob - old_log_probs)\n",
    "\n",
    "            # Finding Surrogate Loss   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -1.0 * torch.min(surr1, surr2) + 0.5 * (state_value - rewards) ** 2 - 0.01 * dist_entropy\n",
    "            loss = loss.mean()\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "        # retun\n",
    "        return np.mean(total_loss)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.policy_old.load_state_dict(torch.load(path))\n",
    "        self.policy.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: torch.Size([64, 4])\n",
      "action: torch.Size([64, 1])\n",
      "log_prob: torch.Size([64, 1])\n",
      "state_value: torch.Size([64, 1])\n",
      "log_prob: torch.Size([64, 1])\n",
      "state_value: torch.Size([64, 1])\n",
      "dist_entropy: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "ppo = PPO(is_continuous_action_space = False, device = \"cpu\")\n",
    "\n",
    "state = torch.randn(64, 4)\n",
    "print(f\"state: {state.shape}\")\n",
    "\n",
    "action, log_prob, state_value = ppo.policy.act(state)\n",
    "print(f\"action: {action.shape}\")\n",
    "print(f\"log_prob: {log_prob.shape}\")\n",
    "print(f\"state_value: {state_value.shape}\")\n",
    "\n",
    "log_prob, state_value, dist_entropy = ppo.policy.evaluate(state, action)\n",
    "print(f\"log_prob: {log_prob.shape}\")\n",
    "print(f\"state_value: {state_value.shape}\")\n",
    "print(f\"dist_entropy: {dist_entropy.shape}\")\n",
    "\n",
    "for i in range(len(state)):\n",
    "    s = state[i:i+1]\n",
    "    ppo.select_action(s)\n",
    "    ppo.buffer.rewards.append(1)\n",
    "    ppo.buffer.is_terminals.append(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python_env\\pytorch\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1600, loss: 0.700, reward: 20.57\n",
      "time step: 3200, loss: 0.501, reward: 28.33\n",
      "time step: 4800, loss: 0.415, reward: 31.22\n",
      "time step: 6400, loss: 0.580, reward: 48.68\n",
      "time step: 8000, loss: 0.400, reward: 61.84\n",
      "time step: 9600, loss: 0.321, reward: 86.06\n",
      "time step: 11200, loss: 0.430, reward: 130.69\n",
      "time step: 12800, loss: 0.463, reward: 167.44\n",
      "time step: 14400, loss: 0.325, reward: 228.71\n",
      "time step: 16000, loss: 0.150, reward: 201.88\n",
      "time step: 17600, loss: 0.017, reward: 180.75\n",
      "time step: 19200, loss: 0.414, reward: 300.40\n",
      "time step: 20800, loss: 0.097, reward: 300.33\n",
      "time step: 22400, loss: -0.940, reward: 348.20\n",
      "time step: 24000, loss: 0.771, reward: 321.25\n",
      "time step: 25600, loss: 0.361, reward: 387.25\n",
      "time step: 27200, loss: 0.729, reward: 400.00\n",
      "time step: 28800, loss: 0.738, reward: 393.60\n",
      "time step: 30400, loss: 0.442, reward: 367.75\n",
      "time step: 32000, loss: 0.174, reward: 385.50\n",
      "time step: 33600, loss: 0.565, reward: 347.00\n",
      "time step: 35200, loss: 0.087, reward: 384.75\n",
      "time step: 36800, loss: 0.664, reward: 388.00\n",
      "time step: 38400, loss: 0.330, reward: 365.00\n",
      "time step: 40000, loss: 0.116, reward: 367.00\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "is_continuous_action_space = False\n",
    "\n",
    "action_std = None\n",
    "min_action_std = None\n",
    "action_std_decay_rate = None\n",
    "action_std_decay_freq = None\n",
    "\n",
    "max_ep_len = 400\n",
    "max_training_timesteps = 40000\n",
    "\n",
    "update_timestep = max_ep_len * 4\n",
    "K_epochs = 40\n",
    "eps_clip = 0.2\n",
    "gamma = 0.99\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.001\n",
    "device = \"cpu\"\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "if is_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "d_model = 64\n",
    "\n",
    "ppo = PPO(\n",
    "    state_dim = state_dim, \n",
    "    action_dim = action_dim, \n",
    "    d_model = d_model,\n",
    "    is_continuous_action_space = is_continuous_action_space,\n",
    "    action_std_init = action_std,\n",
    "    K_epochs = K_epochs,\n",
    "    eps_clip = eps_clip,\n",
    "    gamma = gamma,\n",
    "    lr_actor = lr_actor,\n",
    "    lr_critic = lr_critic,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "time_step = 0\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "while time_step <= max_training_timesteps:    \n",
    "    state, _ = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len + 1):\n",
    "        # select action with policy\n",
    "        action = ppo.select_action(state[None, :])\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo.buffer.rewards.append(reward)\n",
    "        ppo.buffer.is_terminals.append(terminated or truncated)\n",
    "    \n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            loss = ppo.update()\n",
    "\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print(f\"time step: {time_step}, loss: {loss:.3f}, reward: {print_avg_reward:.2f}\")\n",
    "\n",
    "            print_running_reward = 0            \n",
    "            print_running_episodes = 0\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if is_continuous_action_space:\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                ppo.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "env.close()\n",
    "ppo.save(\"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode = \"human\")\n",
    "\n",
    "ppo = PPO(state_dim, action_dim)\n",
    "ppo.load(\"model.pt\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(400):\n",
    "    action = ppo.select_action(state[None, :])\n",
    "    state, reward, terminated, truncated, info = env.step(action.item())\n",
    "    if truncated:\n",
    "        break\n",
    "\n",
    "ppo.buffer.clear()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
